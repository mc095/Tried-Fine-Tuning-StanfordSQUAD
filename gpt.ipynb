{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5fba053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required packages installed successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_packages():\n",
    "    packages = [\"transformers\", \"datasets\", \"peft\", \"torch\", \"accelerate\", \"trl\"]\n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "install_packages()\n",
    "print(\"Required packages installed successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126b71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\GPT\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 2. Load Libraries and Set Up Environment\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4666a818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\GPT\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ganes\\.cache\\huggingface\\hub\\models--google--flan-t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: google/flan-t5-small\n",
      "Approximate parameters: 77.0M\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def load_model():\n",
    "    model_name = \"google/flan-t5-small\"\n",
    "    token = \"hf-token\" \n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer and model with the token\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\", token=token)\n",
    "        \n",
    "        print(f\"Model loaded: {model_name}\")\n",
    "        print(f\"Approximate parameters: {model.num_parameters() / 1_000_000:.1f}M\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "# Call the function to load the model and tokenizer\n",
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4442cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\GPT\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ganes\\.cache\\huggingface\\hub\\datasets--squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 87599/87599 [00:00<00:00, 1172462.24 examples/s]\n",
      "Generating validation split: 100%|██████████| 10570/10570 [00:00<00:00, 962794.39 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 examples from SQuAD dataset\n",
      "\n",
      "Example data:\n",
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front o...\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Answer: Saint Bernadette Soubirous\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_dataset_sample():\n",
    "    token = \"hf-token\"\n",
    "    \n",
    "    try:\n",
    "        # Load a small subset of SQuAD with the token\n",
    "        dataset = load_dataset(\"squad\", split=\"train[:1000]\", token=token)\n",
    "        print(f\"Loaded {len(dataset)} examples from SQuAD dataset\")\n",
    "        \n",
    "        # Show an example\n",
    "        print(\"\\nExample data:\")\n",
    "        example = dataset[0]\n",
    "        print(f\"Context: {example['context'][:150]}...\")\n",
    "        print(f\"Question: {example['question']}\")\n",
    "        print(f\"Answer: {example['answers']['text'][0]}\")\n",
    "        \n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Call the function to load the dataset\n",
    "dataset = load_dataset_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfc793f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 4793.70 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Prepare the Data for Fine-tuning\n",
    "def preprocess_function(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for context, question, answer in zip(examples[\"context\"], examples[\"question\"], examples[\"answers\"]):\n",
    "        # Format: \"Answer the question based on the context: <context> Question: <question>\"\n",
    "        input_text = f\"Answer the question based on the context: {context} Question: {question}\"\n",
    "        target_text = answer[\"text\"][0] if len(answer[\"text\"]) > 0 else \"I don't know.\"\n",
    "        \n",
    "        inputs.append(input_text)\n",
    "        targets.append(target_text)\n",
    "    \n",
    "    # Tokenize inputs and targets\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=64, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "print(f\"Processed {len(tokenized_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model created\n",
      "Trainable parameters: 344,064 (0.45% of total)\n"
     ]
    }
   ],
   "source": [
    "# 6. Set Up PEFT (Parameter-Efficient Fine-Tuning)\n",
    "def create_peft_model(model):\n",
    "    # Set up LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        r=8,                  # Rank of the update matrices\n",
    "        lora_alpha=32,        # Alpha parameter for LoRA scaling\n",
    "        lora_dropout=0.1,     # Dropout probability for LoRA layers\n",
    "        target_modules=[\"q\", \"v\"],  # Modules to apply LoRA to\n",
    "    )\n",
    "    \n",
    "    # Create PEFT model\n",
    "    peft_model = get_peft_model(model, peft_config)\n",
    "    print(\"PEFT model created\")\n",
    "    \n",
    "    # Count trainable parameters to show memory efficiency\n",
    "    trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params / total_params:.2%} of total)\")\n",
    "    \n",
    "    return peft_model\n",
    "\n",
    "peft_model = create_peft_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b37eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ganes\\AppData\\Local\\Temp\\ipykernel_12988\\84415351.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "e:\\GPT\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 40:26, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>48.335400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>47.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>44.899300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>40.714300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>42.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>40.352400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>36.880600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>38.703300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>38.434500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\GPT\\venv\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/google/flan-t5-small/resolve/main/config.json (Request ID: Root=1-681a3332-45074e4f7ced85215b7281b8;b00691b5-aae5-49fa-9fa3-ed002b75fd31)\n",
      "\n",
      "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in google/flan-t5-small.\n",
      "  warnings.warn(\n",
      "e:\\GPT\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in google/flan-t5-small - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "e:\\GPT\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "e:\\GPT\\venv\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/google/flan-t5-small/resolve/main/config.json (Request ID: Root=1-681a3695-230bb87f3ac5f90c68885576;f393a6bb-8a01-4a20-84ad-753c6fd352d9)\n",
      "\n",
      "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in google/flan-t5-small.\n",
      "  warnings.warn(\n",
      "e:\\GPT\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in google/flan-t5-small - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "e:\\GPT\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "e:\\GPT\\venv\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/google/flan-t5-small/resolve/main/config.json (Request ID: Root=1-681a39c7-5d9b67216cb181c06ec34064;becbc1e2-aa8f-4268-89b0-9e6762603449)\n",
      "\n",
      "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in google/flan-t5-small.\n",
      "  warnings.warn(\n",
      "e:\\GPT\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in google/flan-t5-small - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "\n",
    "def train_model(peft_model, tokenized_dataset):\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./finetuned-squad-model\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=1e-4,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_strategy=\"no\",  \n",
    "        fp16=torch.cuda.is_available(), \n",
    "    )\n",
    "    \n",
    "    # Create data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=peft_model,\n",
    "        padding=True,\n",
    "        label_pad_token_id=-100,\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=peft_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# Call the function to train the model\n",
    "trainer = train_model(peft_model, tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ff8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./finetuned-squad-model-final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\GPT\\venv\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/google/flan-t5-small/resolve/main/config.json (Request ID: Root=1-681a39cd-0567c3917ace8d533a8f9a2f;263d5712-adf0-462e-a59c-2a951ebe6ea4)\n",
      "\n",
      "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in google/flan-t5-small.\n",
      "  warnings.warn(\n",
      "e:\\GPT\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in google/flan-t5-small - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 8. Save the Fine-tuned Model\n",
    "def save_model(peft_model, tokenizer):\n",
    "    # Save the model\n",
    "    output_dir = \"./finetuned-squad-model-final\"\n",
    "    peft_model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "save_model(peft_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0ad52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Context (truncated): Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden...\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Expected answer: Saint Bernadette Soubirous\n",
      "Model's answer: Saint-Bélemic\n",
      "\n",
      "Example 2:\n",
      "Context (truncated): Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden...\n",
      "Question: What is in front of the Notre Dame Main Building?\n",
      "Expected answer: a copper statue of Christ\n",
      "Model's answer: the Grotto\n",
      "\n",
      "Example 3:\n",
      "Context (truncated): Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden...\n",
      "Question: The Basilica of the Sacred heart at Notre Dame is beside to which structure?\n",
      "Expected answer: the Main Building\n",
      "Model's answer: St. Bernadette Soubirous\n",
      "\n",
      "Example 4:\n",
      "Context (truncated): Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden...\n",
      "Question: What is the Grotto at Notre Dame?\n",
      "Expected answer: a Marian place of prayer and reflection\n",
      "Model's answer: a Marian place of prayer and reflection\n",
      "\n",
      "Example 5:\n",
      "Context (truncated): Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden...\n",
      "Question: What sits on top of the Main Building at Notre Dame?\n",
      "Expected answer: a golden statue of the Virgin Mary\n",
      "Model's answer: gold dome\n"
     ]
    }
   ],
   "source": [
    "# 9. Test the Fine-tuned Model with Dataset Examples\n",
    "def test_model_with_dataset(peft_model, dataset):\n",
    "    def answer_question(context, question):\n",
    "        input_text = f\"Answer the question based on the context: {context} Question: {question}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = peft_model.generate(\n",
    "                input_ids=inputs.input_ids,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "    # Sample a few examples from the dataset\n",
    "    test_examples = dataset.select(range(5))\n",
    "    \n",
    "    # Test each example\n",
    "    for i, example in enumerate(test_examples):\n",
    "        context = example[\"context\"]\n",
    "        question = example[\"question\"]\n",
    "        expected_answer = example[\"answers\"][\"text\"][0]\n",
    "        \n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Context (truncated): {context[:100]}...\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected answer: {expected_answer}\")\n",
    "        \n",
    "        # Get model's answer\n",
    "        model_answer = answer_question(context, question)\n",
    "        print(f\"Model's answer: {model_answer}\")\n",
    "\n",
    "test_model_with_dataset(peft_model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce858e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      "The university is the major seat of the Congregation of Holy Cross (albeit not its official headquarters, which are in Rome). Its main seminary, Moreau Seminary, is located on the campus across St. Joseph lake from the Main Building. Old College, the oldest building on campus and located near the shore of St. Mary lake, houses undergraduate seminarians. Retired priests and brothers reside in Fatima House (a former retreat center), Holy Cross House, as well as Columba Hall near the Grotto. The university through the Moreau Seminary has ties to theologian Frederick Buechner. While not Catholic, Buechner has praised writers from Notre Dame and Moreau Seminary created a Buechner Prize for Preaching.\n",
      "\n",
      "Answer: Cathedral\n",
      "Answer: a Buechner Prize for Preaching\n",
      "Answer: a crucifix\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Uncomment to run interactive questioning:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43minteractive_qa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36minteractive_qa\u001b[39m\u001b[34m(peft_model)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Interactive question answering\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     user_question = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mAsk a question about the context (or type \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m to exit): \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m user_question.lower() == \u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     28\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GPT\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GPT\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# 10. Interactive Question Answering\n",
    "def interactive_qa(peft_model):\n",
    "    def answer_question(context, question):\n",
    "        input_text = f\"Answer the question based on the context: {context} Question: {question}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = peft_model.generate(\n",
    "                input_ids=inputs.input_ids,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "    # Sample context from our dataset\n",
    "    sample_context = dataset[10][\"context\"]\n",
    "    print(f\"Context:\\n{sample_context}\\n\")\n",
    "    \n",
    "    # Interactive question answering\n",
    "    while True:\n",
    "        user_question = input(\"\\nAsk a question about the context (or type 'quit' to exit): \")\n",
    "        if user_question.lower() == \"quit\":\n",
    "            break\n",
    "            \n",
    "        answer = answer_question(sample_context, user_question)\n",
    "        print(f\"Answer: {answer}\")\n",
    "\n",
    "# Uncomment to run interactive questioning:\n",
    "interactive_qa(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8693f19",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_metric' from 'datasets' (e:\\GPT\\venv\\Lib\\site-packages\\datasets\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_metric\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#------------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 5. Define Metric Computation\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#------------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_metrics\u001b[39m(predictions, labels):\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'load_metric' from 'datasets' (e:\\GPT\\venv\\Lib\\site-packages\\datasets\\__init__.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "# 5. Define Metric Computation\n",
    "\n",
    "def compute_metrics(predictions, labels):\n",
    "    squad_metric = load_metric(\"squad\")\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Format for SQuAD metric\n",
    "    formatted_predictions = [{\"id\": str(i), \"prediction_text\": pred} for i, pred in enumerate(decoded_preds)]\n",
    "    formatted_references = [{\"id\": str(i), \"answers\": {\"text\": [label], \"answer_start\": [0]}} for i, label in enumerate(decoded_labels)]\n",
    "    \n",
    "    # Compute metrics\n",
    "    results = squad_metric.compute(predictions=formatted_predictions, references=formatted_references)\n",
    "    \n",
    "    # Calculate accuracy (exact match of decoded strings)\n",
    "    accuracy = sum(pred.strip().lower() == label.strip().lower() for pred, label in zip(decoded_preds, decoded_labels)) / len(decoded_preds)\n",
    "    \n",
    "    return {\n",
    "        \"exact_match\": results[\"exact_match\"],\n",
    "        \"f1\": results[\"f1\"],\n",
    "        \"accuracy\": accuracy\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3104fc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in e:\\gpt\\venv\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in e:\\gpt\\venv\\lib\\site-packages (from evaluate) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\gpt\\venv\\lib\\site-packages (from evaluate) (2.2.5)\n",
      "Requirement already satisfied: dill in e:\\gpt\\venv\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in e:\\gpt\\venv\\lib\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in e:\\gpt\\venv\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in e:\\gpt\\venv\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in e:\\gpt\\venv\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in e:\\gpt\\venv\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in e:\\gpt\\venv\\lib\\site-packages (from evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in e:\\gpt\\venv\\lib\\site-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in e:\\gpt\\venv\\lib\\site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in e:\\gpt\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in e:\\gpt\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (20.0.0)\n",
      "Requirement already satisfied: aiohttp in e:\\gpt\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.11.18)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\gpt\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\gpt\\venv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\gpt\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\gpt\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\gpt\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\gpt\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
      "Requirement already satisfied: colorama in e:\\gpt\\venv\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\gpt\\venv\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\gpt\\venv\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\gpt\\venv\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in e:\\gpt\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\gpt\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\gpt\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\gpt\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\gpt\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\gpt\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\gpt\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in e:\\gpt\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713b42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Created test set with 50 examples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate  # Use evaluate instead of datasets.load_metric\n",
    "import numpy as np\n",
    "\n",
    "# Use existing variables: peft_model, tokenizer, dataset, device\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Create a Small Test Set from Existing Dataset\n",
    "\n",
    "def create_test_set(dataset, num_examples=50):\n",
    "    # Use a subset of the existing dataset as a test set\n",
    "    test_dataset = dataset.select(range(num_examples))\n",
    "    print(f\"Created test set with {len(test_dataset)} examples\")\n",
    "    return test_dataset\n",
    "\n",
    "test_dataset = create_test_set(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126124c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 1541.76 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 test examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Preprocess Test Data (Reuse Existing Preprocess Function)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for context, question, answer in zip(examples[\"context\"], examples[\"question\"], examples[\"answers\"]):\n",
    "        input_text = f\"Answer the question based on the context: {context} Question: {question}\"\n",
    "        target_text = answer[\"text\"][0] if len(answer[\"text\"]) > 0 else \"I don't know.\"\n",
    "        inputs.append(input_text)\n",
    "        targets.append(target_text)\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=True)\n",
    "    labels = tokenizer(targets, max_length=64, truncation=True, padding=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "print(f\"Processed {len(tokenized_test_dataset)} test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeead89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Define Manual Metric Computation\n",
    "\n",
    "def compute_metrics(predictions, labels):\n",
    "    # wl: Manual computation due to evaluate.load(\"squad\") errors\n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Manual SQuAD metrics\n",
    "    def compute_f1(pred, ref):\n",
    "        pred_tokens = pred.lower().split()\n",
    "        ref_tokens = ref.lower().split()\n",
    "        common = len(set(pred_tokens) & set(ref_tokens))\n",
    "        if common == 0:\n",
    "            return 0.0\n",
    "        precision = common / len(pred_tokens) if pred_tokens else 0.0\n",
    "        recall = common / len(ref_tokens) if ref_tokens else 0.0\n",
    "        return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    # Exact Match: Percentage of exact string matches\n",
    "    exact_match = sum(pred.strip().lower() == ref.strip().lower() for pred, ref in zip(decoded_preds, decoded_labels)) / len(decoded_preds) * 100\n",
    "    \n",
    "    # F1: Average token overlap\n",
    "    f1 = sum(compute_f1(pred, ref) for pred, ref in zip(decoded_preds, decoded_labels)) / len(decoded_preds) * 100\n",
    "    \n",
    "    # Accuracy: Same as EM (exact string match)\n",
    "    accuracy = exact_match\n",
    "    \n",
    "    return {\n",
    "        \"exact_match\": exact_match,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d52b7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "\n",
      "Performance Metrics on Test Set:\n",
      "Exact Match (EM): 54.00%\n",
      "F1 Score: 67.53%\n",
      "Accuracy: 54.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Evaluate Model on Test Set\n",
    "\n",
    "def evaluate_model(peft_model, tokenized_test_dataset):\n",
    "    def generate_predictions(dataset):\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        for example in dataset:\n",
    "            input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = peft_model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            label = tokenizer.decode(example[\"labels\"], skip_special_tokens=True)\n",
    "            predictions.append(pred)\n",
    "            labels.append(label)\n",
    "        return predictions, labels\n",
    "    \n",
    "    print(\"Generating predictions...\")\n",
    "    predictions, labels = generate_predictions(tokenized_test_dataset)\n",
    "    \n",
    "    # Tokenize for metric computation\n",
    "    pred_ids = tokenizer(predictions, max_length=64, truncation=True, padding=True)[\"input_ids\"]\n",
    "    label_ids = tokenizer(labels, max_length=64, truncation=True, padding=True)[\"input_ids\"]\n",
    "    \n",
    "    metrics = compute_metrics(pred_ids, label_ids)\n",
    "    \n",
    "    print(\"\\nPerformance Metrics on Test Set:\")\n",
    "    print(f\"Exact Match (EM): {metrics['exact_match']:.2f}%\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.2f}%\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.2f}%\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "metrics = evaluate_model(peft_model, tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a1a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Original Test Examples:\n",
      "\n",
      "Example 1:\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Expected: Saint Bernadette Soubirous\n",
      "Model: Mary\n",
      "\n",
      "Example 2:\n",
      "Question: What is in front of the Notre Dame Main Building?\n",
      "Expected: a copper statue of Christ\n",
      "Model: the main building\n",
      "\n",
      "Example 3:\n",
      "Question: The Basilica of the Sacred heart at Notre Dame is beside to which structure?\n",
      "Expected: the Main Building\n",
      "Model: Main Building\n",
      "\n",
      "Example 4:\n",
      "Question: What is the Grotto at Notre Dame?\n",
      "Expected: a Marian place of prayer and reflection\n",
      "Model: a replica of the grotto\n",
      "\n",
      "Example 5:\n",
      "Question: What sits on top of the Main Building at Notre Dame?\n",
      "Expected: a golden statue of the Virgin Mary\n",
      "Model: gold dome\n",
      "\n",
      "Original Test Example Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6. Re-evaluate Original Test Examples\n",
    "\n",
    "def evaluate_original_examples(peft_model):\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"context\": \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. The Grotto, a Marian place of prayer and reflection, is a replica of the grotto at Lourdes.\",\n",
    "            \"question\": \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\",\n",
    "            \"expected\": \"Saint Bernadette Soubirous\"\n",
    "        },\n",
    "        {\n",
    "            \"context\": \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary.\",\n",
    "            \"question\": \"What is in front of the Notre Dame Main Building?\",\n",
    "            \"expected\": \"a copper statue of Christ\"\n",
    "        },\n",
    "        {\n",
    "            \"context\": \"Architecturally, the school has a Catholic character. The Basilica of the Sacred Heart is beside the Main Building.\",\n",
    "            \"question\": \"The Basilica of the Sacred heart at Notre Dame is beside to which structure?\",\n",
    "            \"expected\": \"the Main Building\"\n",
    "        },\n",
    "        {\n",
    "            \"context\": \"Architecturally, the school has a Catholic character. The Grotto, a Marian place of prayer and reflection, is a replica of the grotto at Lourdes.\",\n",
    "            \"question\": \"What is the Grotto at Notre Dame?\",\n",
    "            \"expected\": \"a Marian place of prayer and reflection\"\n",
    "        },\n",
    "        {\n",
    "            \"context\": \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary.\",\n",
    "            \"question\": \"What sits on top of the Main Building at Notre Dame?\",\n",
    "            \"expected\": \"a golden statue of the Virgin Mary\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    def answer_question(context, question):\n",
    "        input_text = f\"Answer the question based on the context: {context} Question: {question}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = peft_model.generate(\n",
    "                input_ids=inputs.input_ids,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=False\n",
    "            )\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    correct = 0\n",
    "    print(\"\\nEvaluating Original Test Examples:\")\n",
    "    for i, test in enumerate(test_cases):\n",
    "        model_answer = answer_question(test[\"context\"], test[\"question\"])\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Question: {test['question']}\")\n",
    "        print(f\"Expected: {test['expected']}\")\n",
    "        print(f\"Model: {model_answer}\")\n",
    "        if model_answer.strip().lower() == test[\"expected\"].strip().lower():\n",
    "            correct += 1\n",
    "    accuracy = correct / len(test_cases) * 100\n",
    "    print(f\"\\nOriginal Test Example Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "evaluate_original_examples(peft_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
